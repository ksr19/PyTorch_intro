{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cCcfxoMm2cg"
      },
      "source": [
        "Будем практиковаться на датасете:\n",
        "https://www.kaggle.com/c/avito-demand-prediction\n",
        "\n",
        "Ваша задача:\n",
        "1. Создать Dataset для загрузки данных (используем только числовые данные)\n",
        "2. Обернуть его в Dataloader\n",
        "3. Написать архитектуру сети, которая предсказывает число показов на основании числовых данных (вы всегда можете нагенерить дополнительных факторов). Сеть должна включать BatchNorm слои и Dropout (или НЕ включать, но нужно обосновать)\n",
        "4. Учить будем на функцию потерь с кагла (log RMSE) - нужно её реализовать\n",
        "5. Сравните сходимость Adam, RMSProp и SGD, сделайте вывод по качеству работы модели\n",
        "\n",
        "train-test разделение нужно сделать с помощью sklearn random_state=13, test_size = 0.25\n",
        "\n",
        "Вопросы? в личку @Kinetikm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NVe3edKmOK0",
        "outputId": "ff860fa8-b80c-4b28-f977-c7f81020f718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 74 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 12.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 113 kB 56.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 150 kB 55.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.4 MB/s \n",
            "\u001b[?25h  Building wheel for kaggle-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lxml (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for lxml\u001b[0m\n",
            "\u001b[?25h  Building wheel for PrettyTable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "    Running setup.py install for lxml ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-g40t_68t/lxml_d637aa94411c48a5b9d501f8ee78626b/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-g40t_68t/lxml_d637aa94411c48a5b9d501f8ee78626b/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-39mjej8v/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/lxml Check the logs for full command output.\u001b[0m\n",
            "{\"username\":\"romankosmodemyanskiy\",\"key\":\"bf69d9972f7afdeff6397ce9b56c7814\"}Downloading train.csv.zip to sample_data\n",
            " 98% 313M/318M [00:02<00:00, 133MB/s]\n",
            "100% 318M/318M [00:02<00:00, 128MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "!pip install -q kaggle-cli\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!cat ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download avito-demand-prediction -f train.csv -p sample_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "01JJXEvBuEAA"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp9Nm835m7NM"
      },
      "source": [
        "1. Создать Dataset для загрузки данных (используем только числовые данные)\n",
        "2. Обернуть его в Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Tp8wD4syvuts"
      },
      "outputs": [],
      "source": [
        "class AvitoDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, filepath, train=True):\n",
        "        df = pd.read_csv(filepath)\n",
        "        df['activation_date'] = pd.to_datetime(df['activation_date'])\n",
        "        df['month'] = df['activation_date'].dt.month\n",
        "        df['day'] = df['activation_date'].dt.day\n",
        "        df['weekday'] = df['activation_date'].dt.weekday\n",
        "\n",
        "        y = df['deal_probability']\n",
        "        X = df.drop(columns=['deal_probability'], axis=1).select_dtypes(exclude=['object', 'datetime64[ns]'])\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13, test_size = 0.25)\n",
        "\n",
        "        X_train = X_train.fillna(0)\n",
        "        X_test = X_test.fillna(0)\n",
        "\n",
        "        if train:\n",
        "          self.data = X_train\n",
        "          self.labels = np.array(y_train)\n",
        "        else:\n",
        "          self.data = X_test\n",
        "          self.labels = np.array(y_test)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data.iloc[index, :].values, self.labels[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eYvNGtHfLJ3Y"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024\n",
        "\n",
        "train_dataset = AvitoDataset('sample_data/train.csv.zip')\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
        "                                          shuffle=True, \n",
        "                                          #num_workers=2, \n",
        "                                          drop_last=True)\n",
        "\n",
        "test_dataset = AvitoDataset('sample_data/train.csv.zip', False)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, \n",
        "                                          shuffle=True, \n",
        "                                          #num_workers=2, \n",
        "                                          drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxlhJQmmm_dE"
      },
      "source": [
        "3. Написать архитектуру сети, которая предсказывает число показов на основании числовых данных (вы всегда можете нагенерить дополнительных факторов). Сеть должна включать BatchNorm слои и Dropout (или НЕ включать, но нужно обосновать)\n",
        "4. Учить будем на функцию потерь с кагла (log RMSE) - нужно её реализовать\n",
        "5. Сравните сходимость Adam, RMSProp и SGD, сделайте вывод по качеству работы модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "irg1Jq8sS3QQ"
      },
      "outputs": [],
      "source": [
        "class Perceptron(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, activation=\"relu\"):\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        self.activation = activation\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        if self.activation==\"relu\":\n",
        "            return F.relu(x)\n",
        "        if self.activation==\"sigmoid\":\n",
        "            return torch.sigmoid(x)\n",
        "        raise RuntimeError\n",
        "        \n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    # Пробовал добавить дополнительный слой с последующей нормализацией и дропаутом, но улучшения результата не удалось добиться\n",
        "    # Поэтому вернулся к архитектуре, представленной на уроке\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm1d(input_dim)\n",
        "        self.fc1 = Perceptron(input_dim, hidden_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.dp1 = nn.Dropout(0.2)\n",
        "        self.fc2 = Perceptron(hidden_dim, 1, activation='sigmoid')\n",
        "        self.double()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.bn1(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.dp1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# В соревновании на каггле использовалалсь метрика RMSE, поэтому реализовывал ее\n",
        "class RMSELoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RMSELoss,self).__init__()\n",
        "\n",
        "    def forward(self,x,y):\n",
        "        criterion = nn.MSELoss()\n",
        "        eps = 1e-6\n",
        "        loss = torch.sqrt(criterion(x, y) + eps)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "i7UE3-0yQcZY"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for i, data in enumerate(dataloader):\n",
        "        inputs, labels = data[0], data[1]\n",
        "\n",
        "        # обнуляем градиент\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels.reshape(1024,-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 300 == 0:\n",
        "            loss, current = loss.item(), i * len(inputs)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>8d}/{size:>8d}]\")\n",
        "\n",
        "\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            outputs = model(inputs)\n",
        "            test_loss += loss_fn(outputs, labels.reshape(1024,-1)).item()\n",
        "    test_loss /= num_batches\n",
        "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pxS7r2RBTKa0"
      },
      "outputs": [],
      "source": [
        "net = FeedForward(6, 30)\n",
        "\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)\n",
        "criterion = RMSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHawwrB7SkYg",
        "outputId": "022cb980-12d2-4dea-f206-67f7a7ddc940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.481468  [       0/ 1127568]\n",
            "loss: 0.463155  [  307200/ 1127568]\n",
            "loss: 0.442622  [  614400/ 1127568]\n",
            "loss: 0.427547  [  921600/ 1127568]\n",
            "Test Error: \n",
            " Avg loss: 0.417899 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.416945  [       0/ 1127568]\n",
            "loss: 0.406108  [  307200/ 1127568]\n",
            "loss: 0.397721  [  614400/ 1127568]\n",
            "loss: 0.388491  [  921600/ 1127568]\n",
            "Test Error: \n",
            " Avg loss: 0.371687 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.370910  [       0/ 1127568]\n",
            "loss: 0.362188  [  307200/ 1127568]\n",
            "loss: 0.354867  [  614400/ 1127568]\n",
            "loss: 0.346678  [  921600/ 1127568]\n",
            "Test Error: \n",
            " Avg loss: 0.334741 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.333797  [       0/ 1127568]\n",
            "loss: 0.332681  [  307200/ 1127568]\n",
            "loss: 0.326494  [  614400/ 1127568]\n",
            "loss: 0.319778  [  921600/ 1127568]\n",
            "Test Error: \n",
            " Avg loss: 0.309056 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.312948  [       0/ 1127568]\n",
            "loss: 0.310885  [  307200/ 1127568]\n",
            "loss: 0.295701  [  614400/ 1127568]\n",
            "loss: 0.295022  [  921600/ 1127568]\n",
            "Test Error: \n",
            " Avg loss: 0.291048 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_data_loader, net, criterion, optimizer)\n",
        "    test(test_data_loader, net, criterion)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем построить модели с другими оптимизаторами:"
      ],
      "metadata": {
        "id": "z8Mxr8vYj_Ck"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "aFFGGz7FVA_4"
      },
      "outputs": [],
      "source": [
        "net = FeedForward(6, 30)\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters())\n",
        "criterion = RMSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB4ZsIoSFiQI",
        "outputId": "23d5d733-1361-4570-dc52-188ca7c55c9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.464453  [       0/ 1127568]\n",
            "loss: 0.257404  [  307200/ 1127568]\n",
            "loss: 0.254053  [  614400/ 1127568]\n",
            "loss: 0.246998  [  921600/ 1127568]\n",
            "Test Error: \n",
            " Avg loss: 0.253009 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.251381  [       0/ 1127568]\n",
            "loss: 0.244445  [  307200/ 1127568]\n",
            "loss: 0.247974  [  614400/ 1127568]\n",
            "loss: 0.240892  [  921600/ 1127568]\n",
            "Test Error: \n",
            " Avg loss: 0.251903 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.250829  [       0/ 1127568]\n",
            "loss: 0.249453  [  307200/ 1127568]\n",
            "loss: 0.255675  [  614400/ 1127568]\n",
            "loss: 0.258463  [  921600/ 1127568]\n",
            "Test Error: \n",
            " Avg loss: 0.252016 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.245201  [       0/ 1127568]\n",
            "loss: 0.249977  [  307200/ 1127568]\n",
            "loss: 0.251528  [  614400/ 1127568]\n",
            "loss: 0.244056  [  921600/ 1127568]\n",
            "Test Error: \n",
            " Avg loss: 0.251075 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.251464  [       0/ 1127568]\n",
            "loss: 0.246134  [  307200/ 1127568]\n",
            "loss: 0.259794  [  614400/ 1127568]\n",
            "loss: 0.257970  [  921600/ 1127568]\n",
            "Test Error: \n",
            " Avg loss: 0.250936 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_data_loader, net, criterion, optimizer)\n",
        "    test(test_data_loader, net, criterion)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = FeedForward(6, 30)\n",
        "\n",
        "optimizer = torch.optim.RMSprop(net.parameters(), lr=1e-3)\n",
        "criterion = RMSELoss()\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_data_loader, net, criterion, optimizer)\n",
        "    test(test_data_loader, net, criterion)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P78-7fZ6plzg",
        "outputId": "3ccb36da-5141-4d89-8faf-cd580d7206df"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.446455  [       0/ 1127568]\n",
            "loss: 0.267454  [  307200/ 1127568]\n",
            "loss: 0.249551  [  614400/ 1127568]\n",
            "loss: 0.236385  [  921600/ 1127568]\n",
            "Test Error: \n",
            " Avg loss: 0.251909 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.251708  [       0/ 1127568]\n",
            "loss: 0.272383  [  307200/ 1127568]\n",
            "loss: 0.264237  [  614400/ 1127568]\n",
            "loss: 0.251765  [  921600/ 1127568]\n",
            "Test Error: \n",
            " Avg loss: 0.252839 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.244552  [       0/ 1127568]\n",
            "loss: 0.243975  [  307200/ 1127568]\n",
            "loss: 0.244089  [  614400/ 1127568]\n",
            "loss: 0.256386  [  921600/ 1127568]\n",
            "Test Error: \n",
            " Avg loss: 0.251878 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.247568  [       0/ 1127568]\n",
            "loss: 0.248801  [  307200/ 1127568]\n",
            "loss: 0.252165  [  614400/ 1127568]\n",
            "loss: 0.247766  [  921600/ 1127568]\n",
            "Test Error: \n",
            " Avg loss: 0.252917 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.245207  [       0/ 1127568]\n",
            "loss: 0.249938  [  307200/ 1127568]\n",
            "loss: 0.246531  [  614400/ 1127568]\n",
            "loss: 0.256342  [  921600/ 1127568]\n",
            "Test Error: \n",
            " Avg loss: 0.251145 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxIGYaS4nG08"
      },
      "source": [
        "RMSprop и Adam сходятся быстрее, чем SGD. Результаты также довольно близки. Однако в сравнении с лидерами рейтинга соревнования на каггле видно, что качество модели довольно низкое. Это неудивительно с учетом небольшого количества признаков, на основе которых проводится обучение."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}